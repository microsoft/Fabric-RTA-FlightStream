{"cells":[{"cell_type":"markdown","source":["![logo](https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31)\n","\n","# **Fabric**\n","### Simulating streaming data for Realtime Analytics âš¡ using fabric Data Engineering notebook \n","### AKA: \"The Wood Chipper\" \n","This notebook will read any CSV you give it via the \"SampleCsv\" Parameter and will send it to an EventStream custom app endpoint (event hub). The notebook will send a certain number of lines for each batches according to the \"MyBatchSize\" Parameter. The number of batch size is computed automatically according to the total number of lines and batch size and the while loop will stop once the file has been streamed completely.  "],"metadata":{},"id":"a93be367-5dd5-41a0-96d2-6db4178a2dc0"},{"cell_type":"markdown","source":["### **0. Set the parameters**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4dad526e-407d-4541-b7d2-d6af28447239"},{"cell_type":"code","source":["# The connection string is what you get from the \"custom app\" endpoint in EventStream\n","MyConnectionString = ''\n","\n","# This is the endpoint for where the CSV file is sitting.\n","SampleCsv = 'abfss://3582b164-c42f-4707-98ac-a85e3bf6a734@msit-onelake.dfs.fabric.microsoft.com/31ba9dc7-0bc3-4f63-9b4b-ffdadd957944/Files/QoS_data/QueenOfTheSky_ex.csv'\n","\n","# Set batch size (i.e. number of rows from the CSV that being sent at once. use a higher number when wanting a more rapid movement on the report)\n","MyBatchSize = 12\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f5198be6-199b-4b75-ae83-27a76d5079bb","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-23T16:06:56.496096Z","session_start_time":"2023-10-23T16:06:57.4028237Z","execution_start_time":"2023-10-23T16:07:13.6712605Z","execution_finish_time":"2023-10-23T16:07:16.3084547Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"aafb7ccf-a107-45c3-ae95-2390f3a316a9"},"text/plain":"StatementMeta(, f5198be6-199b-4b75-ae83-27a76d5079bb, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d894de5-a5a5-4dc8-b48d-5267651ab5a4"},{"cell_type":"markdown","source":["### **1. Install dependencies and Event Hub library**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bcd9c142-f852-4c63-8070-3254b142edb1"},{"cell_type":"code","source":["pip install azure-eventhub>=5.11.0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f5198be6-199b-4b75-ae83-27a76d5079bb","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-23T16:06:56.5016589Z","session_start_time":null,"execution_start_time":"2023-10-23T16:07:16.7225492Z","execution_finish_time":"2023-10-23T16:07:26.6018643Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a0fa8905-7755-46f9-af12-774ab7468a4b"},"text/plain":"StatementMeta(, f5198be6-199b-4b75-ae83-27a76d5079bb, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e09adb7-282c-474d-b31b-712a9d3273ee"},{"cell_type":"code","source":["import time\n","import os\n","import datetime\n","import json\n","import math\n","from azure.eventhub import EventHubProducerClient, EventData"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f5198be6-199b-4b75-ae83-27a76d5079bb","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-23T16:06:56.5094743Z","session_start_time":null,"execution_start_time":"2023-10-23T16:07:27.0592616Z","execution_finish_time":"2023-10-23T16:07:28.3775156Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"77a2db7b-4ae1-46b1-9298-922258e90ef3"},"text/plain":"StatementMeta(, f5198be6-199b-4b75-ae83-27a76d5079bb, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c1fe989f-60a1-4c78-83dc-f18235f0771e"},{"cell_type":"markdown","source":["### **2. Create a Python script to send events to your event stream**\n","\n","ref: https://learn.microsoft.com/azure/event-hubs/event-hubs-capture-python#create-a-python-script-to-send-events-to-your-event-hub"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e2da62d-0b89-4cd7-8652-d17c1bc422e0"},{"cell_type":"code","source":["#Read in the CSV to a dataframe. \n","df = spark.read.csv(path=SampleCsv,header=True)\n","\n","#Instantiate an event hub producer\n","producer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n","\n","# Determine the row count of the file\n","z = df.count() \n","\n","#Set some control variables\n","i            = MyBatchSize \n","x            = 0     # We open the batch at the first row by array index so we stat at 0\n","y            = x+i   # We seal the batch at Start + Increment(i)\n","BatchCounter = 0     # Initializing a batch counter\n","RowCounter   = 0     # Initializine a Row counter\n","TargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\n","print ('====================================')\n","print ('Target batch count should be: '+ str(TargetBatchCount))\n","print ('====================================')\n","print ('Beginning stream...')\n","print ('====================================')\n","\n","while BatchCounter < TargetBatchCount:\n","\n","    BatchCounter = BatchCounter + 1 # == Mouve our batch counter one notch up  \n","    b = producer.create_batch()     # == Instantiate the batch\n","    j = df.toJSON().collect()[x:y]  # == Collect Rows from x to y and convert them to JSON\n","    for ii in range(0, len(j)):     # == We have to add every row in the batch individually to the event hub payload so Kusto can read it in.    \n","        b.add(EventData(j[ii]))     # == Add the JSON to the payload\n","    producer.send_batch(b)          # == Send the batch to Event hub!\n","    time.sleep(1)                   # == We add an intentional 1s pause\n","    producer.close()                # == Clean up the batch\n","    # Printing some stats to track the stream                \n","    print ('This was batch #:' + str(BatchCounter))\n","    print ('We loaded rows from: ' + str(x) + ' to row: ' + str(y)) \n","    #Setting the control variable for the next pass\n","    RowCounter   = RowCounter + i\n","    RowRemaining = max(0,(z-RowCounter))\n","    x = y\n","    y = x+i if RowRemaining > i else x+RowRemaining\n","    print ('Rows remaining in the stream: ' + str(RowRemaining))\n","    print ('====================================')\n","\n","print ('====================================')  \n","print ('End of stream reached')\n","print ('====================================')    \n","print ('Number of batches was: '  + str(BatchCounter))\n","print ('Last batch was from row: '+ str(x) + ' to row: '+ str(y)) \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f5198be6-199b-4b75-ae83-27a76d5079bb","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-23T16:06:56.5164944Z","session_start_time":null,"execution_start_time":"2023-10-23T16:07:28.7562561Z","execution_finish_time":"2023-10-23T16:12:53.6237448Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":151,"UNKNOWN":0},"jobs":[{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":158,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:49.061GMT","completionTime":"2023-10-23T16:12:49.205GMT","stageIds":[163],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":157,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:46.902GMT","completionTime":"2023-10-23T16:12:47.067GMT","stageIds":[162],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":156,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:44.798GMT","completionTime":"2023-10-23T16:12:44.943GMT","stageIds":[161],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":155,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:42.781GMT","completionTime":"2023-10-23T16:12:42.922GMT","stageIds":[160],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":154,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:40.698GMT","completionTime":"2023-10-23T16:12:40.847GMT","stageIds":[159],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":153,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:38.577GMT","completionTime":"2023-10-23T16:12:38.724GMT","stageIds":[158],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":152,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:36.479GMT","completionTime":"2023-10-23T16:12:36.622GMT","stageIds":[157],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":151,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:34.423GMT","completionTime":"2023-10-23T16:12:34.565GMT","stageIds":[156],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":150,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:32.347GMT","completionTime":"2023-10-23T16:12:32.492GMT","stageIds":[155],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":149,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:30.226GMT","completionTime":"2023-10-23T16:12:30.378GMT","stageIds":[154],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":148,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:28.148GMT","completionTime":"2023-10-23T16:12:28.293GMT","stageIds":[153],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":147,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:26.113GMT","completionTime":"2023-10-23T16:12:26.271GMT","stageIds":[152],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":146,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:24.017GMT","completionTime":"2023-10-23T16:12:24.169GMT","stageIds":[151],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":145,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:21.871GMT","completionTime":"2023-10-23T16:12:22.020GMT","stageIds":[150],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":144,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:19.767GMT","completionTime":"2023-10-23T16:12:19.911GMT","stageIds":[149],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":143,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:17.703GMT","completionTime":"2023-10-23T16:12:17.845GMT","stageIds":[148],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":142,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:15.634GMT","completionTime":"2023-10-23T16:12:15.777GMT","stageIds":[147],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":141,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:13.602GMT","completionTime":"2023-10-23T16:12:13.747GMT","stageIds":[146],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":140,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:11.488GMT","completionTime":"2023-10-23T16:12:11.632GMT","stageIds":[145],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_9641/2785492844.py:27","dataWritten":0,"dataRead":138003,"rowCount":1766,"usageDescription":"","jobId":139,"name":"collect at /tmp/ipykernel_9641/2785492844.py:27","description":"Job group for statement 6:\n#Read in the CSV to a dataframe. \ndf = spark.read.csv(path=SampleCsv,header=True)\n\n#Instantiate an event hub producer\nproducer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n\n# Determine the row count of the file\nz = df.count() \n\n#Set some control variables\ni            = MyBatchSize \nx            = 0     # We open the batch at the first row by array index so we stat at 0\ny            = x+i   # We seal the batch at Start + Increment(i)\nBatchCounter = 0     # Initializing a batch counter\nRowCounter   = 0     # Initializine a Row counter\nTargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\nprint ('====================================')\nprint ('Target batch count should be: '+ str(TargetBatchCount))\nprint ('====================================')\nprint ('Beginning stream...')\nprint ('====================================')\n\nwhile BatchCounter < TargetBatchCount:\n\n    BatchCounter = BatchCounter + 1 #...","submissionTime":"2023-10-23T16:12:09.444GMT","completionTime":"2023-10-23T16:12:09.595GMT","stageIds":[144],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"43424585-9e51-47e6-9ffe-7d3adc0c4e60"},"text/plain":"StatementMeta(, f5198be6-199b-4b75-ae83-27a76d5079bb, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["====================================\nTarget batch count should be: 148\n====================================\nBeginning stream...\n====================================\nThis was batch #:1\nWe loaded rows from: 0 to row: 12\nRows remaining in the stream: 1754\n====================================\nThis was batch #:2\nWe loaded rows from: 12 to row: 24\nRows remaining in the stream: 1742\n====================================\nThis was batch #:3\nWe loaded rows from: 24 to row: 36\nRows remaining in the stream: 1730\n====================================\nThis was batch #:4\nWe loaded rows from: 36 to row: 48\nRows remaining in the stream: 1718\n====================================\nThis was batch #:5\nWe loaded rows from: 48 to row: 60\nRows remaining in the stream: 1706\n====================================\nThis was batch #:6\nWe loaded rows from: 60 to row: 72\nRows remaining in the stream: 1694\n====================================\nThis was batch #:7\nWe loaded rows from: 72 to row: 84\nRows remaining in the stream: 1682\n====================================\nThis was batch #:8\nWe loaded rows from: 84 to row: 96\nRows remaining in the stream: 1670\n====================================\nThis was batch #:9\nWe loaded rows from: 96 to row: 108\nRows remaining in the stream: 1658\n====================================\nThis was batch #:10\nWe loaded rows from: 108 to row: 120\nRows remaining in the stream: 1646\n====================================\nThis was batch #:11\nWe loaded rows from: 120 to row: 132\nRows remaining in the stream: 1634\n====================================\nThis was batch #:12\nWe loaded rows from: 132 to row: 144\nRows remaining in the stream: 1622\n====================================\nThis was batch #:13\nWe loaded rows from: 144 to row: 156\nRows remaining in the stream: 1610\n====================================\nThis was batch #:14\nWe loaded rows from: 156 to row: 168\nRows remaining in the stream: 1598\n====================================\nThis was batch #:15\nWe loaded rows from: 168 to row: 180\nRows remaining in the stream: 1586\n====================================\nThis was batch #:16\nWe loaded rows from: 180 to row: 192\nRows remaining in the stream: 1574\n====================================\nThis was batch #:17\nWe loaded rows from: 192 to row: 204\nRows remaining in the stream: 1562\n====================================\nThis was batch #:18\nWe loaded rows from: 204 to row: 216\nRows remaining in the stream: 1550\n====================================\nThis was batch #:19\nWe loaded rows from: 216 to row: 228\nRows remaining in the stream: 1538\n====================================\nThis was batch #:20\nWe loaded rows from: 228 to row: 240\nRows remaining in the stream: 1526\n====================================\nThis was batch #:21\nWe loaded rows from: 240 to row: 252\nRows remaining in the stream: 1514\n====================================\nThis was batch #:22\nWe loaded rows from: 252 to row: 264\nRows remaining in the stream: 1502\n====================================\nThis was batch #:23\nWe loaded rows from: 264 to row: 276\nRows remaining in the stream: 1490\n====================================\nThis was batch #:24\nWe loaded rows from: 276 to row: 288\nRows remaining in the stream: 1478\n====================================\nThis was batch #:25\nWe loaded rows from: 288 to row: 300\nRows remaining in the stream: 1466\n====================================\nThis was batch #:26\nWe loaded rows from: 300 to row: 312\nRows remaining in the stream: 1454\n====================================\nThis was batch #:27\nWe loaded rows from: 312 to row: 324\nRows remaining in the stream: 1442\n====================================\nThis was batch #:28\nWe loaded rows from: 324 to row: 336\nRows remaining in the stream: 1430\n====================================\nThis was batch #:29\nWe loaded rows from: 336 to row: 348\nRows remaining in the stream: 1418\n====================================\nThis was batch #:30\nWe loaded rows from: 348 to row: 360\nRows remaining in the stream: 1406\n====================================\nThis was batch #:31\nWe loaded rows from: 360 to row: 372\nRows remaining in the stream: 1394\n====================================\nThis was batch #:32\nWe loaded rows from: 372 to row: 384\nRows remaining in the stream: 1382\n====================================\nThis was batch #:33\nWe loaded rows from: 384 to row: 396\nRows remaining in the stream: 1370\n====================================\nThis was batch #:34\nWe loaded rows from: 396 to row: 408\nRows remaining in the stream: 1358\n====================================\nThis was batch #:35\nWe loaded rows from: 408 to row: 420\nRows remaining in the stream: 1346\n====================================\nThis was batch #:36\nWe loaded rows from: 420 to row: 432\nRows remaining in the stream: 1334\n====================================\nThis was batch #:37\nWe loaded rows from: 432 to row: 444\nRows remaining in the stream: 1322\n====================================\nThis was batch #:38\nWe loaded rows from: 444 to row: 456\nRows remaining in the stream: 1310\n====================================\nThis was batch #:39\nWe loaded rows from: 456 to row: 468\nRows remaining in the stream: 1298\n====================================\nThis was batch #:40\nWe loaded rows from: 468 to row: 480\nRows remaining in the stream: 1286\n====================================\nThis was batch #:41\nWe loaded rows from: 480 to row: 492\nRows remaining in the stream: 1274\n====================================\nThis was batch #:42\nWe loaded rows from: 492 to row: 504\nRows remaining in the stream: 1262\n====================================\nThis was batch #:43\nWe loaded rows from: 504 to row: 516\nRows remaining in the stream: 1250\n====================================\nThis was batch #:44\nWe loaded rows from: 516 to row: 528\nRows remaining in the stream: 1238\n====================================\nThis was batch #:45\nWe loaded rows from: 528 to row: 540\nRows remaining in the stream: 1226\n====================================\nThis was batch #:46\nWe loaded rows from: 540 to row: 552\nRows remaining in the stream: 1214\n====================================\nThis was batch #:47\nWe loaded rows from: 552 to row: 564\nRows remaining in the stream: 1202\n====================================\nThis was batch #:48\nWe loaded rows from: 564 to row: 576\nRows remaining in the stream: 1190\n====================================\nThis was batch #:49\nWe loaded rows from: 576 to row: 588\nRows remaining in the stream: 1178\n====================================\nThis was batch #:50\nWe loaded rows from: 588 to row: 600\nRows remaining in the stream: 1166\n====================================\nThis was batch #:51\nWe loaded rows from: 600 to row: 612\nRows remaining in the stream: 1154\n====================================\nThis was batch #:52\nWe loaded rows from: 612 to row: 624\nRows remaining in the stream: 1142\n====================================\nThis was batch #:53\nWe loaded rows from: 624 to row: 636\nRows remaining in the stream: 1130\n====================================\nThis was batch #:54\nWe loaded rows from: 636 to row: 648\nRows remaining in the stream: 1118\n====================================\nThis was batch #:55\nWe loaded rows from: 648 to row: 660\nRows remaining in the stream: 1106\n====================================\nThis was batch #:56\nWe loaded rows from: 660 to row: 672\nRows remaining in the stream: 1094\n====================================\nThis was batch #:57\nWe loaded rows from: 672 to row: 684\nRows remaining in the stream: 1082\n====================================\nThis was batch #:58\nWe loaded rows from: 684 to row: 696\nRows remaining in the stream: 1070\n====================================\nThis was batch #:59\nWe loaded rows from: 696 to row: 708\nRows remaining in the stream: 1058\n====================================\nThis was batch #:60\nWe loaded rows from: 708 to row: 720\nRows remaining in the stream: 1046\n====================================\nThis was batch #:61\nWe loaded rows from: 720 to row: 732\nRows remaining in the stream: 1034\n====================================\nThis was batch #:62\nWe loaded rows from: 732 to row: 744\nRows remaining in the stream: 1022\n====================================\nThis was batch #:63\nWe loaded rows from: 744 to row: 756\nRows remaining in the stream: 1010\n====================================\nThis was batch #:64\nWe loaded rows from: 756 to row: 768\nRows remaining in the stream: 998\n====================================\nThis was batch #:65\nWe loaded rows from: 768 to row: 780\nRows remaining in the stream: 986\n====================================\nThis was batch #:66\nWe loaded rows from: 780 to row: 792\nRows remaining in the stream: 974\n====================================\nThis was batch #:67\nWe loaded rows from: 792 to row: 804\nRows remaining in the stream: 962\n====================================\nThis was batch #:68\nWe loaded rows from: 804 to row: 816\nRows remaining in the stream: 950\n====================================\nThis was batch #:69\nWe loaded rows from: 816 to row: 828\nRows remaining in the stream: 938\n====================================\nThis was batch #:70\nWe loaded rows from: 828 to row: 840\nRows remaining in the stream: 926\n====================================\nThis was batch #:71\nWe loaded rows from: 840 to row: 852\nRows remaining in the stream: 914\n====================================\nThis was batch #:72\nWe loaded rows from: 852 to row: 864\nRows remaining in the stream: 902\n====================================\nThis was batch #:73\nWe loaded rows from: 864 to row: 876\nRows remaining in the stream: 890\n====================================\nThis was batch #:74\nWe loaded rows from: 876 to row: 888\nRows remaining in the stream: 878\n====================================\nThis was batch #:75\nWe loaded rows from: 888 to row: 900\nRows remaining in the stream: 866\n====================================\nThis was batch #:76\nWe loaded rows from: 900 to row: 912\nRows remaining in the stream: 854\n====================================\nThis was batch #:77\nWe loaded rows from: 912 to row: 924\nRows remaining in the stream: 842\n====================================\nThis was batch #:78\nWe loaded rows from: 924 to row: 936\nRows remaining in the stream: 830\n====================================\nThis was batch #:79\nWe loaded rows from: 936 to row: 948\nRows remaining in the stream: 818\n====================================\nThis was batch #:80\nWe loaded rows from: 948 to row: 960\nRows remaining in the stream: 806\n====================================\nThis was batch #:81\nWe loaded rows from: 960 to row: 972\nRows remaining in the stream: 794\n====================================\nThis was batch #:82\nWe loaded rows from: 972 to row: 984\nRows remaining in the stream: 782\n====================================\nThis was batch #:83\nWe loaded rows from: 984 to row: 996\nRows remaining in the stream: 770\n====================================\nThis was batch #:84\nWe loaded rows from: 996 to row: 1008\nRows remaining in the stream: 758\n====================================\nThis was batch #:85\nWe loaded rows from: 1008 to row: 1020\nRows remaining in the stream: 746\n====================================\nThis was batch #:86\nWe loaded rows from: 1020 to row: 1032\nRows remaining in the stream: 734\n====================================\nThis was batch #:87\nWe loaded rows from: 1032 to row: 1044\nRows remaining in the stream: 722\n====================================\nThis was batch #:88\nWe loaded rows from: 1044 to row: 1056\nRows remaining in the stream: 710\n====================================\nThis was batch #:89\nWe loaded rows from: 1056 to row: 1068\nRows remaining in the stream: 698\n====================================\nThis was batch #:90\nWe loaded rows from: 1068 to row: 1080\nRows remaining in the stream: 686\n====================================\nThis was batch #:91\nWe loaded rows from: 1080 to row: 1092\nRows remaining in the stream: 674\n====================================\nThis was batch #:92\nWe loaded rows from: 1092 to row: 1104\nRows remaining in the stream: 662\n====================================\nThis was batch #:93\nWe loaded rows from: 1104 to row: 1116\nRows remaining in the stream: 650\n====================================\nThis was batch #:94\nWe loaded rows from: 1116 to row: 1128\nRows remaining in the stream: 638\n====================================\nThis was batch #:95\nWe loaded rows from: 1128 to row: 1140\nRows remaining in the stream: 626\n====================================\nThis was batch #:96\nWe loaded rows from: 1140 to row: 1152\nRows remaining in the stream: 614\n====================================\nThis was batch #:97\nWe loaded rows from: 1152 to row: 1164\nRows remaining in the stream: 602\n====================================\nThis was batch #:98\nWe loaded rows from: 1164 to row: 1176\nRows remaining in the stream: 590\n====================================\nThis was batch #:99\nWe loaded rows from: 1176 to row: 1188\nRows remaining in the stream: 578\n====================================\nThis was batch #:100\nWe loaded rows from: 1188 to row: 1200\nRows remaining in the stream: 566\n====================================\nThis was batch #:101\nWe loaded rows from: 1200 to row: 1212\nRows remaining in the stream: 554\n====================================\nThis was batch #:102\nWe loaded rows from: 1212 to row: 1224\nRows remaining in the stream: 542\n====================================\nThis was batch #:103\nWe loaded rows from: 1224 to row: 1236\nRows remaining in the stream: 530\n====================================\nThis was batch #:104\nWe loaded rows from: 1236 to row: 1248\nRows remaining in the stream: 518\n====================================\nThis was batch #:105\nWe loaded rows from: 1248 to row: 1260\nRows remaining in the stream: 506\n====================================\nThis was batch #:106\nWe loaded rows from: 1260 to row: 1272\nRows remaining in the stream: 494\n====================================\nThis was batch #:107\nWe loaded rows from: 1272 to row: 1284\nRows remaining in the stream: 482\n====================================\nThis was batch #:108\nWe loaded rows from: 1284 to row: 1296\nRows remaining in the stream: 470\n====================================\nThis was batch #:109\nWe loaded rows from: 1296 to row: 1308\nRows remaining in the stream: 458\n====================================\nThis was batch #:110\nWe loaded rows from: 1308 to row: 1320\nRows remaining in the stream: 446\n====================================\nThis was batch #:111\nWe loaded rows from: 1320 to row: 1332\nRows remaining in the stream: 434\n====================================\nThis was batch #:112\nWe loaded rows from: 1332 to row: 1344\nRows remaining in the stream: 422\n====================================\nThis was batch #:113\nWe loaded rows from: 1344 to row: 1356\nRows remaining in the stream: 410\n====================================\nThis was batch #:114\nWe loaded rows from: 1356 to row: 1368\nRows remaining in the stream: 398\n====================================\nThis was batch #:115\nWe loaded rows from: 1368 to row: 1380\nRows remaining in the stream: 386\n====================================\nThis was batch #:116\nWe loaded rows from: 1380 to row: 1392\nRows remaining in the stream: 374\n====================================\nThis was batch #:117\nWe loaded rows from: 1392 to row: 1404\nRows remaining in the stream: 362\n====================================\nThis was batch #:118\nWe loaded rows from: 1404 to row: 1416\nRows remaining in the stream: 350\n====================================\nThis was batch #:119\nWe loaded rows from: 1416 to row: 1428\nRows remaining in the stream: 338\n====================================\nThis was batch #:120\nWe loaded rows from: 1428 to row: 1440\nRows remaining in the stream: 326\n====================================\nThis was batch #:121\nWe loaded rows from: 1440 to row: 1452\nRows remaining in the stream: 314\n====================================\nThis was batch #:122\nWe loaded rows from: 1452 to row: 1464\nRows remaining in the stream: 302\n====================================\nThis was batch #:123\nWe loaded rows from: 1464 to row: 1476\nRows remaining in the stream: 290\n====================================\nThis was batch #:124\nWe loaded rows from: 1476 to row: 1488\nRows remaining in the stream: 278\n====================================\nThis was batch #:125\nWe loaded rows from: 1488 to row: 1500\nRows remaining in the stream: 266\n====================================\nThis was batch #:126\nWe loaded rows from: 1500 to row: 1512\nRows remaining in the stream: 254\n====================================\nThis was batch #:127\nWe loaded rows from: 1512 to row: 1524\nRows remaining in the stream: 242\n====================================\nThis was batch #:128\nWe loaded rows from: 1524 to row: 1536\nRows remaining in the stream: 230\n====================================\nThis was batch #:129\nWe loaded rows from: 1536 to row: 1548\nRows remaining in the stream: 218\n====================================\nThis was batch #:130\nWe loaded rows from: 1548 to row: 1560\nRows remaining in the stream: 206\n====================================\nThis was batch #:131\nWe loaded rows from: 1560 to row: 1572\nRows remaining in the stream: 194\n====================================\nThis was batch #:132\nWe loaded rows from: 1572 to row: 1584\nRows remaining in the stream: 182\n====================================\nThis was batch #:133\nWe loaded rows from: 1584 to row: 1596\nRows remaining in the stream: 170\n====================================\nThis was batch #:134\nWe loaded rows from: 1596 to row: 1608\nRows remaining in the stream: 158\n====================================\nThis was batch #:135\nWe loaded rows from: 1608 to row: 1620\nRows remaining in the stream: 146\n====================================\nThis was batch #:136\nWe loaded rows from: 1620 to row: 1632\nRows remaining in the stream: 134\n====================================\nThis was batch #:137\nWe loaded rows from: 1632 to row: 1644\nRows remaining in the stream: 122\n====================================\nThis was batch #:138\nWe loaded rows from: 1644 to row: 1656\nRows remaining in the stream: 110\n====================================\nThis was batch #:139\nWe loaded rows from: 1656 to row: 1668\nRows remaining in the stream: 98\n====================================\nThis was batch #:140\nWe loaded rows from: 1668 to row: 1680\nRows remaining in the stream: 86\n====================================\nThis was batch #:141\nWe loaded rows from: 1680 to row: 1692\nRows remaining in the stream: 74\n====================================\nThis was batch #:142\nWe loaded rows from: 1692 to row: 1704\nRows remaining in the stream: 62\n====================================\nThis was batch #:143\nWe loaded rows from: 1704 to row: 1716\nRows remaining in the stream: 50\n====================================\nThis was batch #:144\nWe loaded rows from: 1716 to row: 1728\nRows remaining in the stream: 38\n====================================\nThis was batch #:145\nWe loaded rows from: 1728 to row: 1740\nRows remaining in the stream: 26\n====================================\nThis was batch #:146\nWe loaded rows from: 1740 to row: 1752\nRows remaining in the stream: 14\n====================================\nThis was batch #:147\nWe loaded rows from: 1752 to row: 1764\nRows remaining in the stream: 2\n====================================\nThis was batch #:148\nWe loaded rows from: 1764 to row: 1766\nRows remaining in the stream: 0\n====================================\n====================================\nEnd of stream reached\n====================================\nNumber of batches was: 148\nLast batch was from row: 1766 to row: 1766\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"61509439-23b7-4247-905e-384ca8d9ecdf"}],"metadata":{"widgets":{},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[]}}},"nbformat":4,"nbformat_minor":5}