{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![logo](https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31)\n","\n","# **Fabric**\n","### Simulating streaming data for Realtime Analytics ⚡ using fabric Data Engineering notebook \n","### \"Flight Streamer\" ✈️ (Python) \n","This notebook is used to take a CSV file extract that contains the complete flight track of one commercial flight that travelled from Toronto, On Canada to Calgary, Ab Canada. Every line of the CSV (1260 rows) represents a plane sighting. i.e. the aircraft transponder signal was detected by an open-source plan tracker and they recorded important data about the aircraft such as heading, velocity, longitude and latitude. The granularity of the data is one reading every 10 seconds.\n","\n","The notebook will stream one row at at time (roughly 1 row/sec) into the custom app endpoint of the Event Stream. This will then be sent to the KQL database. The cell Marked \"0\" is where you set your parameters. We've populated it with our endpoint address but do make sure you replace it with yours."]},{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["The data used here was sourced from: \n","\n","The OpenSky Network, http://www.opensky-network.org \n","\n","or by citing the paper\n","\n","Bringing up OpenSky: A large-scale ADS-B sensor network for research\n","Matthias Schäfer, Martin Strohmeier, Vincent Lenders, Ivan Martinovic, Matthias Wilhelm\n","ACM/IEEE International Conference on Information Processing in Sensor Networks, April 2014\n","\n","(ii) You agree to expunge any and all copies of the received data set(s) upon completion or termination of stated research and/or termination of data access or use. Completion of stated research shall allow for a reasonable period of time that You may need to retain the data set(s) in order to satisfy scientific reproducibility obligations."]},{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **0. Set my connection string**"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["MyConnectionString = ''\n","\n","SampleCsv = 'abfss://3582b164-c42f-4707-98ac-a85e3bf6a734@msit-onelake.dfs.fabric.microsoft.com/f8a13080-852c-40d5-8181-c2bba079a710/Files/Sample/FlightsSample.csv'"]},{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **1. Install dependencies and Event Hub library**"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["pip install azure-eventhub>=5.11.0"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import time\n","import os\n","import datetime\n","import json\n","from azure.eventhub import EventHubProducerClient, EventData"]},{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### **2. Create a Python script to send events to your event stream**\n","\n","ref: https://learn.microsoft.com/azure/event-hubs/event-hubs-capture-python#create-a-python-script-to-send-events-to-your-event-hub"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["df = spark.read.csv(path=SampleCsv,header=True)\n","\n","producer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n","\n","z = df.count() \n","\n","# df.collect()[0:5]\n","# df.collect()[5:10]\n","\n","x = 0\n","y = 1\n","\n","for x in range(0, z): \n","    b = producer.create_batch() \n","    j = df.toJSON().collect()[x:y]\n","    # print(j)\n","    b.add(EventData(j))\n","    producer.send_batch(b) # send it!\n","    time.sleep(1)\n","    producer.close()\n","    x=y\n","    y=y+1\n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"known_lakehouses":[]}},"widgets":{}},"nbformat":4,"nbformat_minor":0}
